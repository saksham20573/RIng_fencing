{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "K-1cUr6CgquA",
        "UIJckiNAgvag"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Some of the code provided in this implementation has been incorporated into the model https://aneesha.medium.com/natural-language-queries-for-any-database-table-with-zero-shot-roberta-based-sql-query-generation-51df57c449e2"
      ],
      "metadata": {
        "id": "KfyK6i1hsQez"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9w-6dCgSgKK5"
      },
      "outputs": [],
      "source": [
        "# Install libs\n",
        "!pip install tableschema\n",
        "!pip install sqlalchemy\n",
        "!pip install records\n",
        "!pip install transformers\n",
        "!pip install sentence-transformers\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports\n",
        "import csv\n",
        "from tableschema import infer\n",
        "import io\n",
        "import pandas as pd\n",
        "from sqlalchemy import create_engine, MetaData, Table, Column, Integer, String\n",
        "from google.colab import files\n",
        "from google.colab import data_table\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "from sklearn import preprocessing\n",
        "from sklearn.svm import SVC\n",
        "from datetime import date\n",
        "from transformers import BertTokenizer, BertForTokenClassification, pipeline\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "import unicodedata\n",
        "import spacy\n",
        "from difflib import SequenceMatcher\n",
        "# from ner import Parser\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Load Google Collab Extensions\n",
        "%load_ext google.colab.data_table"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wzOgiB3DgPd6",
        "outputId": "7115e152-0ca8-4095-9bc4-64221002a822"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/cuda/__init__.py:497: UserWarning: Can't initialize NVML\n",
            "  warnings.warn(\"Can't initialize NVML\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download en_core_web_trf"
      ],
      "metadata": {
        "id": "XLdk9XZHitOu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy-transformers"
      ],
      "metadata": {
        "id": "8VGb4opMngW4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Upload the sample schema file\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "RDY6WibBgThf",
        "outputId": "1d03a3f9-5135-4a38-d760-7a11f466eb55"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-fb176d17-f5cc-455b-aaae-cb8fb5279f49\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-fb176d17-f5cc-455b-aaae-cb8fb5279f49\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving account.csv to account.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Path to dataset\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/Final Ring fencing dataset - Sheet1.csv\")"
      ],
      "metadata": {
        "id": "06WTmAo0gaqh"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Evaluate"
      ],
      "metadata": {
        "id": "K-1cUr6CgquA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_feature_model2(data_frame):\n",
        "  \"\"\"\n",
        "  Input a data frame and return the embedding vectors for the each sentence column using model2,\n",
        "  Return 2 matrices each of shape (#_samples, #size_of_word_emb).\n",
        "  \"\"\"\n",
        "# sentence-transformers/all-distilroberta-v1\n",
        "  non_cont_model2 = SentenceTransformer('distilbert-base-uncased')\n",
        "  \n",
        "  feature1 = non_cont_model2.encode(data_frame)\n",
        "  \n",
        "  return feature1\n",
        "\n",
        "def classify_query(Query):\n",
        "  df = pd.read_csv(\"/content/drive/MyDrive/Final Ring fencing dataset - Sheet1.csv\")\n",
        "  column = \"LABEL\"\n",
        "  df_enc = df.copy()\n",
        "  for i in df_enc.index:\n",
        "    if df[column][i] == \"WITHDRAW\":\n",
        "        df_enc[column][i] = 0\n",
        "    elif df[column][i] == \"DEPOSIT\":\n",
        "        df_enc[column][i] = 1\n",
        "    else:\n",
        "        df_enc[column][i] = 2\n",
        "\n",
        "  df_enc = df_enc.sample(frac = 1)\n",
        "  X = df_enc[\"ENTRY\"]\n",
        "  y = df_enc[\"LABEL\"]\n",
        "  y=y.astype('int')\n",
        "  # X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)\n",
        "\n",
        "  feature_1_train = get_feature_model2(np.array(X))\n",
        "\n",
        "  model_classify = LogisticRegression(max_iter = 500)\n",
        "  model_classify.fit(np.array(feature_1_train), y)\n",
        "\n",
        "\n",
        "  Query_type = model_classify.predict(get_feature_model2(Query).reshape(1, -1))\n",
        "  # .reshape(1, -1)\n",
        "  # print(Query_type)[0]\n",
        "  print(Query_type)\n",
        "  return Query_type\n",
        "\n",
        "def similar(a, b):\n",
        "    return SequenceMatcher(None, a, b).ratio()\n",
        "\n",
        "def break_query(query, purpose_list):\n",
        "  model = SentenceTransformer('bert-base-nli-mean-tokens')\n",
        "  # !python -m spacy download en_core_web_trf\n",
        "  nlp = spacy.load(\"en_core_web_trf\")\n",
        "\n",
        "  # query = \"How much money can I spend for buying a car\"\n",
        "  query = unicodedata.normalize('NFKD', query)\n",
        "  print(\"=================================\")\n",
        "  print(\"Before breaking\")\n",
        "  print(\"=================================\")\n",
        "  print(query)\n",
        "\n",
        "  lower_query = query.lower()\n",
        "  q_purpose = \"\"\n",
        "  \n",
        "  break_ind = -1\n",
        "  tags = []\n",
        "  tokens = []\n",
        "  purpose_found = False\n",
        "\n",
        "  # POS tagging\n",
        "  doc = nlp(query)\n",
        "  for token in doc:\n",
        "      tokens.append(str(token))\n",
        "      tags.append((str(token), token.pos_))\n",
        "\n",
        "  f = -1\n",
        "  # Extracting purpose and breaking\n",
        "  for purpose in purpose_list:\n",
        "    p = purpose.lower()\n",
        "    doc_purp = nlp(purpose)\n",
        "    token_purp = []\n",
        "    for i in doc_purp:\n",
        "      token_purp.append(str(i))\n",
        "    ngram = len(token_purp)\n",
        "    for i in range(len(tokens) - 1, ngram - 1, -1):\n",
        "      str1 = \" \".join(tokens[i - ngram + 1: i + 1])\n",
        "      str2 = \" \".join(token_purp)\n",
        "      samples = [str1, str2]\n",
        "      sentence_embeddings = model.encode(samples)\n",
        "      sim = cosine_similarity([sentence_embeddings[0]], sentence_embeddings[1:])\n",
        "      if float(sim[0][0]) > 0.9:\n",
        "        f = i - ngram + 1\n",
        "    if(f == -1):\n",
        "      continue\n",
        "    break\n",
        "\n",
        "  # If purpose not found\n",
        "  if f == -1:\n",
        "    return \"General Purpose\", query  \n",
        "  \n",
        "  # Purpose found\n",
        "  purpose_found = True\n",
        "  print(\"Purpose = \" + purpose)\n",
        "  print()\n",
        "\n",
        "  # Breaking the query\n",
        "  doc = nlp(query)\n",
        "  seg1 = \"\"\n",
        "\n",
        "\n",
        "  # Rule based query division\n",
        "  for i in range(len(doc) - 1, -1, -1):\n",
        "    # print(i)\n",
        "    if tags[i][1] == \"ADP\" or tags[i][0].lower() == \"to\":\n",
        "        \n",
        "      # Printing break word\n",
        "      print(\"Break at \\\"\" + str(tags[i][0]) + \"\\\"\")\n",
        "      \n",
        "      print(\"=================================\")\n",
        "      print(\"After breaking\")\n",
        "      print(\"=================================\")\n",
        "      # Printing the first segement\n",
        "      print(\"First Segment:\", end=\" \")\n",
        "      print(tokens[:i])\n",
        "      # segement 1 to be sent to execute section for query generation\n",
        "      seg1 = tokens[:i]\n",
        "\n",
        "      # Printing the Second Segement\n",
        "      print(\"Second segment:\", end=\" \")\n",
        "      print(tokens[i:])\n",
        "      #  segment 2 to be sent to the lookup data structure with the purpose p\n",
        "      seg2 = tokens[i:]\n",
        "      break\n",
        "  \n",
        "  return purpose, seg1\n",
        "\n",
        "def run_evaluate(Query, purpose_list, userID):\n",
        "  purpose, q2 = break_query(Query, purpose_list)\n",
        "  q2s = q2.copy()\n",
        "  q2s.extend([\"from accountNo\", str(userID)])\n",
        "  q2s = \" \".join(q2s)\n",
        "  Query_type = classify_query(Query)\n",
        "  return purpose, q2, q2s, Query_type"
      ],
      "metadata": {
        "id": "YP9YJi08ggIZ"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Enforce"
      ],
      "metadata": {
        "id": "UIJckiNAgvag"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def withdraw_money(user, purpose, Lookup_table, users):\n",
        "  # global Lookup_table\n",
        "  perms = Lookup_table[purpose]\n",
        "  user_perms = perms[0][user.pan_card]\n",
        "  if not user_perms.withdraw_perm:\n",
        "    print(\"Failed! you cannot withdraw money for this purpose\")\n",
        "    return -1\n",
        "  todays_date = date.today()\n",
        "  if todays_date.year < user_perms.time_withdraw:\n",
        "    print(\"Failed! you cannot withdraw money for this purpose at the present time\")\n",
        "    return -1\n",
        "  if user_perms.cap_withdraw != -1:\n",
        "    print(\"Success! Withdrawing money for {0}\".format(purpose))\n",
        "    return user_perms.cap_withdraw\n",
        "  print(\"Success! Withdrawing money for {0}\".format(purpose))\n",
        "  return perms[1]\n",
        "\n",
        "def view_money(user, purpose, Lookup_table, users):\n",
        "  # global Lookup_table\n",
        "  perms = Lookup_table[purpose]\n",
        "  user_perms = perms[0][user.pan_card]\n",
        "  if not user_perms.view_perm:\n",
        "    print(\"Failed! you cannot view money for this purpose\")\n",
        "    return -1\n",
        "  print(\"Success! You can view money for this purpose.\")\n",
        "  return 1\n",
        "\n",
        "def deposit_money(user, purpose, Lookup_table, users):\n",
        "  # global Lookup_table\n",
        "  perms = Lookup_table[purpose]\n",
        "  user_perms = perms[0][user.pan_card]\n",
        "  if not user_perms.deposit_perm:\n",
        "    print(\"Failed! you cannot deposit money for this purpose\")\n",
        "    return -1\n",
        "  todays_date = date.today()\n",
        "  if todays_date.year < user_perms.time_deposit:\n",
        "    print(\"Failed! you cannot deposit money for this purpose at the present time\")\n",
        "    return -1\n",
        "  # if user_perms.cap_deposit != -1 and amount > user_perms.cap_deposit:\n",
        "  #   print(\"Failed! You only have Rs {0} for this purpose.\".format(user_perms.cap_deposit))\n",
        "  #   return user_perms.cap_deposit\n",
        "\n",
        "  if user_perms.cap_deposit != -1:\n",
        "    print(\"Success! depositing money for {0}\".format(purpose))\n",
        "    return user_perms.cap_deposit\n",
        "  print(\"Success! depositing money for {0}\".format(purpose))\n",
        "  return perms[1]\n",
        "\n",
        "\n",
        "def run_enforce(Query, curr_user, userID, users, Lookup_table, purpose, Query_type):\n",
        "    # global users\n",
        "    # global Lookup_table\n",
        "    # users = users\n",
        "    # Lookup_table = Lookup_table\n",
        "    # tokenizer = BertTokenizer.from_pretrained(\"QCRI/bert-base-multilingual-cased-pos-english\")\n",
        "    # model_pos = BertForTokenClassification.from_pretrained(\"QCRI/bert-base-multilingual-cased-pos-english\")\n",
        "    # class_pipe = pipeline(model = \"QCRI/bert-base-multilingual-cased-pos-english\", tokenizer = \"QCRI/bert-base-multilingual-cased-pos-english\", task = \"token-classification\")\n",
        "    \n",
        "    if Query_type == 0:\n",
        "        output = withdraw_money(curr_user, purpose, Lookup_table, users)\n",
        "    elif Query_type == 1:\n",
        "        output = deposit_money(curr_user, purpose, Lookup_table, users)\n",
        "    else:\n",
        "        output = view_money(curr_user, purpose, Lookup_table, users)\n",
        "    return output"
      ],
      "metadata": {
        "id": "fJvnLHk0guPQ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Execute"
      ],
      "metadata": {
        "id": "EWh2EOMMg2Ma"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def view_execute(q2s, output):\n",
        "  # Path to the NLP2sql models\n",
        "  path_wikisql = \"/content/drive/My Drive/NLP2SQLmodels\"\n",
        "  sqlite_db = create_engine('sqlite://',echo=False)\n",
        "\n",
        "  uploaded_files = list(uploaded.keys())\n",
        "\n",
        "  uploaded_file = None\n",
        "  schema_types = []\n",
        "  field_names = []\n",
        "\n",
        "  if len(uploaded_files) > 0 :\n",
        "    uploaded_file = uploaded_files[0]\n",
        "    schema = getSchema(uploaded_file)\n",
        "    schema_types = schema['schema_types']\n",
        "    field_names = schema['field_names']\n",
        "\n",
        "    # Add data to in memory sqllite database\n",
        "    with open(uploaded_file, 'r') as file:\n",
        "      data_df = pd.read_csv(file)\n",
        "      data_df.to_sql('uploadedtable', con=sqlite_db, index=True, index_label='uploaded_id', if_exists='replace')\n",
        "  else:\n",
        "    print('No file has been uploaded')\n",
        "\n",
        "  # Adapted from  https://colab.research.google.com/drive/1qYJTbbEXYFVdY6xae9Zmt96hkeW8ZFrn but with the training and testing removed\n",
        "\n",
        "  !rm -rf RoBERTa-NL2SQL\n",
        "\n",
        "  GIT_PATH = \"https://github.com/aneesha/RoBERTa-NL2SQL\"\n",
        "  !git clone \"{GIT_PATH}\"\n",
        "  %cd RoBERTa-NL2SQL\n",
        "\n",
        "  import load_data\n",
        "  import torch\n",
        "  import json,argparse\n",
        "  import load_model\n",
        "  import roberta_training\n",
        "  import corenlp_local\n",
        "  import seq2sql_model_testing\n",
        "  import seq2sql_model_training_functions\n",
        "  import model_save_and_infer\n",
        "  import dev_function\n",
        "  import infer_functions\n",
        "  import time\n",
        "  import os\n",
        "  import nltk\n",
        "\n",
        "  from dbengine_sqlnet import DBEngine\n",
        "  from torchsummary import summary\n",
        "  from tqdm.notebook import tqdm\n",
        "  nltk.download('punkt')\n",
        "  from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "  import warnings\n",
        "  warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "  device = torch.device(\"cuda\")\n",
        "\n",
        "  # load models\n",
        "  roberta_model, tokenizer, configuration = load_model.get_roberta_model()          # Loads the RoBERTa Model\n",
        "  seq2sql_model = load_model.get_seq2sql_model(configuration.hidden_size) \n",
        "\n",
        "  path_roberta_pretrained = path_wikisql + \"/model_roberta_best.pt\"\n",
        "  path_model_pretrained = path_wikisql + \"/model_best.pt\"\n",
        "\n",
        "  # load pre-trained weights\n",
        "  if torch.cuda.is_available():\n",
        "      res = torch.load(path_roberta_pretrained)\n",
        "  else:\n",
        "      res = torch.load(path_roberta_pretrained, map_location='cpu')\n",
        "\n",
        "  roberta_model.load_state_dict(res['model_roberta'])\n",
        "\n",
        "  if torch.cuda.is_available():\n",
        "      res = torch.load(path_model_pretrained)\n",
        "  else:\n",
        "      res = torch.load(path_model_pretrained, map_location='cpu')\n",
        "\n",
        "  seq2sql_model.load_state_dict(res['model'])\n",
        "  table_id = 'uploadedtable'\n",
        "\n",
        "  natural_language_query = q2s\n",
        "  domainswap =['score']\n",
        "\n",
        "  if 'score' in natural_language_query:\n",
        "    natural_language_query = natural_language_query.replace('score', 'value')\n",
        "\n",
        "  sqlqueries = infer_functions.infer(\n",
        "                  natural_language_query,\n",
        "                  table_id, field_names, schema_types, tokenizer, \n",
        "                  seq2sql_model, roberta_model, configuration, max_seq_length=222,\n",
        "                  num_target_layers=2,\n",
        "                  beam_size=4\n",
        "              )\n",
        "\n",
        "  sqlquery = sqlqueries[0]\n",
        "  print('Generated SQL: ',sqlquery)\n",
        "\n",
        "  aggs = ['count', 'avg', 'max', 'min', 'distinct']\n",
        "  uniquelist = ['distinct','unique']\n",
        "\n",
        "\n",
        "  if any([x in natural_language_query for x in uniquelist]):\n",
        "    sqlquery = sqlquery.replace('SELECT ', 'SELECT distinct ')\n",
        "\n",
        "  if not any(x in sqlquery for x in aggs):\n",
        "    sqlquery = sqlquery.replace('SELECT ', 'SELECT *, ')\n",
        "\n",
        "  print('Postprocessed SQL: ',sqlquery)\n",
        "\n",
        "  df = pd.read_sql(sqlquery, sqlite_db)\n",
        "  data_table.DataTable(df, include_index=False, num_rows_per_page=20)\n",
        "\n",
        "def getSchema(filename):\n",
        "  schema_types = []\n",
        "  field_names = []\n",
        "  schema = infer(filename, limit=500, headers=1, confidence=0.85)\n",
        "  field_objs = schema['fields']\n",
        "  for field in field_objs:\n",
        "    field_names.append(field['name'])\n",
        "    schema_type = field['type']\n",
        "    if schema_type == 'string':\n",
        "      schema_types.append('text')\n",
        "    else:\n",
        "      schema_types.append('real')\n",
        "  return {'schema_types': schema_types,'field_names':field_names}\n",
        "\n",
        "\n",
        "def withdraw_execute(Query, table, acc, amount):\n",
        "  NER = spacy.load(\"en_core_web_trf\")\n",
        "  text1= NER(Query)\n",
        "  amount_q = 0\n",
        "  for word in text1.ents:\n",
        "    if word.label_ == \"MONEY\" or word.label_ == \"CARDINAL\":\n",
        "      amount_q = int(word.text)\n",
        "      break\n",
        "  if(amount == -1):\n",
        "    print(\"Query cannot be executed\")\n",
        "    return\n",
        "  if amount_q > amount:\n",
        "    print(\"Max amount limit for withdrawl exceeded. Updating the amount.\")\n",
        "  amount = amount_q\n",
        "  withdraw_stencil = \"update \" + str(table) + \" set balance = balance - \" + str(amount) + \" where accountNo = \" + str(acc)\n",
        "  print(\"Final query: \" + withdraw_stencil)\n",
        "  return\n",
        "\n",
        "def deposit_execute(Query, table, acc, amount):\n",
        "  NER = spacy.load(\"en_core_web_trf\")\n",
        "  text1= NER(Query)\n",
        "  amount_q = 0\n",
        "  for word in text1.ents:\n",
        "    if word.label_ == \"MONEY\" or word.label_ == \"CARDINAL\":\n",
        "      amount_q = int(word.text)\n",
        "      break\n",
        "  if(amount == -1):\n",
        "    print(\"Query cannot be executed\")\n",
        "    return\n",
        "  if amount_q > amount:\n",
        "    print(\"Max amount limit for withdrawl exceeded. Updating the amount.\")\n",
        "  amount = amount_q\n",
        "  deposit_stencil = \"update \" + str(table) + \" set balance = balance + \" + str(amount) + \" where accountNo = \" + str(acc)\n",
        "  print(\"Final query: \" + deposit_stencil)\n",
        "  return\n",
        "\n",
        "\n",
        "def run_execute(q2, q2s, userID, output, Query_type):\n",
        "    if Query_type == 0:\n",
        "        withdraw_execute(\" \".join(q2), \"user_table\", userID, output)\n",
        "    elif Query_type == 1:\n",
        "        deposit_execute(\" \".join(q2), \"user_table\", userID, output)\n",
        "    else:\n",
        "        output = view_execute(q2s, output)"
      ],
      "metadata": {
        "id": "B0VJTP5Hg3oz"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code to run"
      ],
      "metadata": {
        "id": "1fhAWNvpiH35"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "global users\n",
        "global Lookup_table\n",
        "users = []\n",
        "Lookup_table = {}\n",
        "def add_user(name, pan_no, relation, acc_no = -1):\n",
        "  temp = User(name, pan_no, relation, acc_no)\n",
        "  global users\n",
        "  users.append(temp)\n",
        "  return temp\n",
        "\n",
        "# Creating the User class\n",
        "class User:\n",
        "  def __init__(self, name, pan_card, relation, acc_no = -1):\n",
        "    \n",
        "    # Initializing permissions\n",
        "    self.name = name\n",
        "    self.pan_card = pan_card\n",
        "    self.relation = relation\n",
        "    self.acc_no = acc_no\n",
        "\n",
        "class UserPermissions:\n",
        "  def __init__(self, u_pan, withdraw_perm = True, deposit_perm = True, view_perm = True, time_deposit = 0, time_withdraw = 0, cap_deposit = -1, cap_withdraw = -1):\n",
        "    self.u_pan = u_pan\n",
        "    self.withdraw_perm = withdraw_perm \n",
        "    self.deposit_perm = deposit_perm\n",
        "    self.time_withdraw = time_withdraw \n",
        "    self.time_deposit = time_deposit \n",
        "    self.cap_withdraw = cap_withdraw \n",
        "    self.cap_deposit = cap_deposit\n",
        "    self.view_perm = view_perm\n",
        "\n",
        "# Setting up the data\n",
        "\n",
        "root = add_user(\"Father\", 1, \"self\")\n",
        "wife = add_user(\"Wife\", 2, \"wife\")\n",
        "daughter = add_user(\"Daughter\", 3, \"daughter\")\n",
        "son = add_user(\"Son\", 4, \"son\")\n",
        "\n",
        "purpose1 = \"Groceries\"\n",
        "purpose2 = \"Self car\"\n",
        "purpose3 = \"Wife's car\"\n",
        "purpose4 = \"Son's college\"\n",
        "purpose5 = \"Daughter's college\"\n",
        "purpose6 = \"General Purpose\"\n",
        "purp_list = [purpose1, purpose2, purpose3, purpose4, purpose5, purpose6]\n",
        "\n",
        "user_dict = {}\n",
        "\n",
        "\n",
        "# Filling the lookup table\n",
        "for i in users:\n",
        "    if i.pan_card == 3 or i.pan_card == 4:\n",
        "        user_dict[i.pan_card] = UserPermissions(i.pan_card, withdraw_perm = False, view_perm = False, deposit_perm = False)\n",
        "    else:\n",
        "        user_dict[i.pan_card] = UserPermissions(i.pan_card)\n",
        "Lookup_table[purpose1] = (user_dict, 50000)\n",
        "\n",
        "\n",
        "user_dict = {}\n",
        "\n",
        "for i in users:\n",
        "    if i.pan_card == 3 or i.pan_card == 4 or i.pan_card == 2:\n",
        "        user_dict[i.pan_card] = UserPermissions(i.pan_card, withdraw_perm = False, view_perm = False, deposit_perm = False)\n",
        "    else:\n",
        "        user_dict[i.pan_card] = UserPermissions(i.pan_card)\n",
        "Lookup_table[purpose2] = (user_dict, 100000)\n",
        "\n",
        "user_dict = {}\n",
        "\n",
        "for i in users:\n",
        "    if i.pan_card == 3 or i.pan_card == 4 or i.pan_card == 1:\n",
        "        user_dict[i.pan_card] = UserPermissions(i.pan_card, withdraw_perm = False, view_perm = False, deposit_perm = False)\n",
        "    else:\n",
        "        user_dict[i.pan_card] = UserPermissions(i.pan_card)\n",
        "Lookup_table[purpose3] = (user_dict, 100000)\n",
        "\n",
        "user_dict = {}\n",
        "for i in users:\n",
        "    if i.pan_card == 3:\n",
        "        user_dict[i.pan_card] = UserPermissions(i.pan_card, deposit_perm = False)\n",
        "    if i.pan_card == 1 or i.pan_card == 2:\n",
        "        user_dict[i.pan_card] = UserPermissions(i.pan_card, withdraw_perm = False)\n",
        "    else:\n",
        "        user_dict[i.pan_card] = UserPermissions(i.pan_card)\n",
        "\n",
        "Lookup_table[purpose4] = (user_dict, 8000)\n",
        "\n",
        "user_dict = {}\n",
        "for i in users:\n",
        "    if i.pan_card == 4:\n",
        "        user_dict[i.pan_card] = UserPermissions(i.pan_card, deposit_perm = False)\n",
        "    if i.pan_card == 1 or i.pan_card == 2:\n",
        "        user_dict[i.pan_card] = UserPermissions(i.pan_card, withdraw_perm = False)\n",
        "    else:\n",
        "        user_dict[i.pan_card] = UserPermissions(i.pan_card)\n",
        "\n",
        "Lookup_table[purpose5] = (user_dict, 12000)\n",
        "\n",
        "user_dict = {}\n",
        "for i in users:\n",
        "    user_dict[i.pan_card] = UserPermissions(i.pan_card)\n",
        "\n",
        "Lookup_table[purpose6] = (user_dict, 100000)\n",
        "\n"
      ],
      "metadata": {
        "id": "AiZkXqCKiHtk"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Query = \"Withdraw 100000 for my daughter's college fees\"\n",
        "print(\"Query to be used: \" + Query)\n",
        "userID = 1111111\n",
        "purpose, q2, q2s, Query_type = run_evaluate(Query, purp_list, userID)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6zXRKDG5jIxQ",
        "outputId": "6bcb813d-2255-4663-f46e-47cf21113aab"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query to be used: Withdraw 100000 for my daughter's college fees\n",
            "=================================\n",
            "Before breaking\n",
            "=================================\n",
            "Withdraw 100000 for my daughter's college fees\n",
            "Purpose = Daughter's college\n",
            "\n",
            "Break at \"for\"\n",
            "=================================\n",
            "After breaking\n",
            "=================================\n",
            "First Segment: ['Withdraw', '100000']\n",
            "Second segment: ['for', 'my', 'daughter', \"'s\", 'college', 'fees']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:sentence_transformers.SentenceTransformer:No sentence-transformers model found with name /root/.cache/torch/sentence_transformers/distilbert-base-uncased. Creating a new one with MEAN pooling.\n",
            "Some weights of the model checkpoint at /root/.cache/torch/sentence_transformers/distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_layer_norm.weight']\n",
            "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "WARNING:sentence_transformers.SentenceTransformer:No sentence-transformers model found with name /root/.cache/torch/sentence_transformers/distilbert-base-uncased. Creating a new one with MEAN pooling.\n",
            "Some weights of the model checkpoint at /root/.cache/torch/sentence_transformers/distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_layer_norm.weight']\n",
            "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = run_enforce(Query, daughter, userID, users, Lookup_table, purpose, Query_type)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tmYTMe_pp1Ko",
        "outputId": "74015312-2d63-4710-dd73-ede10e62340e"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Success! Withdrawing money for Daughter's college\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "run_execute(q2, q2s, userID, outputs, Query_type)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uu7k0hnupz3n",
        "outputId": "6c44890d-6291-45c5-b78b-31c861c77448"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Max amount limit for withdrawl exceeded. Updating the amount.\n",
            "Final query: update user_table set balance = balance - 100000 where accountNo = 1111111\n"
          ]
        }
      ]
    }
  ]
}